{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c631459",
   "metadata": {},
   "source": [
    "# Recurrent SNN Plus Learnable Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e37600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "from tonic import DiskCachedDataset\n",
    "from tonic.dataset import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from collections.abc import Callable\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6299c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([30, 16, 2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "sensor_size = tonic.datasets.DVSGesture.sensor_size\n",
    "\n",
    "# 15 time steps\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToFrame(sensor_size=sensor_size, n_time_bins=30),\n",
    "])\n",
    "\n",
    "train_set = tonic.datasets.DVSGesture(save_to='./data', train=True, transform=transform)\n",
    "test_set = tonic.datasets.DVSGesture(save_to='./data', train=False, transform=transform)\n",
    "\n",
    "# Dataloaders\n",
    "cached_dataloader_args = {\n",
    "    \"batch_size\": 16,\n",
    "    \"collate_fn\": tonic.collation.PadTensors(batch_first=False), \n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 2,\n",
    "    \"pin_memory\": True\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train_set, **cached_dataloader_args)\n",
    "test_loader = DataLoader(test_set, **cached_dataloader_args)\n",
    "\n",
    "data, targets = next(iter(train_loader))\n",
    "print(f\"Data shape: {data.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea967fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: xpu\n"
     ]
    }
   ],
   "source": [
    "if torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f31b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron parameters\n",
    "beta = 0.5  \n",
    "spike_grad = surrogate.atan() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d81a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNN(nn.Module):\n",
    "    def __init__(self, beta=0.5, threshold=1.0):\n",
    "        super(RSNN, self).__init__()\n",
    "\n",
    "        # Surrogate gradient for backprop\n",
    "        spike_grad = surrogate.atan()\n",
    "\n",
    "        # --- Layer 1: Conv -> BN -> Pool -> Learnable LIF ---\n",
    "        # Input: 2 channels, 128x128\n",
    "        self.conv1 = nn.Conv2d(2, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2) # Output: 16 x 64 x 64\n",
    "        self.lif1 = snn.Leaky(beta=beta, threshold=threshold, \n",
    "                              learn_beta=True, learn_threshold=True,\n",
    "                              spike_grad=spike_grad, init_hidden=True)\n",
    "\n",
    "        # --- Layer 2: Conv -> BN -> Pool -> Learnable LIF ---\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2) # Output: 32 x 32 x 32\n",
    "        self.lif2 = snn.Leaky(beta=beta, threshold=threshold, \n",
    "                              learn_beta=True, learn_threshold=True,\n",
    "                              spike_grad=spike_grad, init_hidden=True)\n",
    "\n",
    "        # --- Layer 3: Conv -> BN -> Pool -> Learnable LIF (New Depth) ---\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2) # Output: 64 x 16 x 16\n",
    "        self.lif3 = snn.Leaky(beta=beta, threshold=threshold, \n",
    "                              learn_beta=True, learn_threshold=True,\n",
    "                              spike_grad=spike_grad, init_hidden=True)\n",
    "                              \n",
    "        # --- Layer 4: Conv -> BN -> Pool -> Learnable LIF (New Depth) ---\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool4 = nn.MaxPool2d(2) # Output: 128 x 8 x 8\n",
    "        self.lif4 = snn.Leaky(beta=beta, threshold=threshold, \n",
    "                              learn_beta=True, learn_threshold=True,\n",
    "                              spike_grad=spike_grad, init_hidden=True)\n",
    "\n",
    "        # --- Layer 5: Flatten -> Linear -> Recurrent LIF (RNN) ---\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(128 * 8 * 8, 11) # 11 Classes\n",
    "        \n",
    "        # RLeaky adds a recurrent connection to the output neurons\n",
    "        # all_to_all=True connects every output neuron to every other in the next step\n",
    "        self.rlif = snn.RLeaky(beta=beta, threshold=threshold, \n",
    "                               learn_beta=True, learn_threshold=True,\n",
    "                               spike_grad=spike_grad, init_hidden=True,\n",
    "                               linear_features=11, all_to_all=True, output=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the spatial layers\n",
    "        x = self.pool1(self.bn1(self.conv1(x)))\n",
    "        x = self.lif1(x)\n",
    "        \n",
    "        x = self.pool2(self.bn2(self.conv2(x)))\n",
    "        x = self.lif2(x)\n",
    "        \n",
    "        x = self.pool3(self.bn3(self.conv3(x)))\n",
    "        x = self.lif3(x)\n",
    "        \n",
    "        x = self.pool4(self.bn4(self.conv4(x)))\n",
    "        x = self.lif4(x)\n",
    "        \n",
    "        # Flatten and pass to Recurrent Layer\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        spk_out, mem_out = self.rlif(x)\n",
    "\n",
    "        return spk_out, mem_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0460c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 188041 parameters.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "net = RSNN().to(device)\n",
    "\n",
    "# Updated Optimizer (Parameters are now learnable, so we pass net.parameters())\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
    "loss_fn = SF.ce_rate_loss()\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in net.parameters())} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf900823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "hist = {\"loss\": [], \"acc\": []}\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    iter_loss = 0\n",
    "    iter_acc = 0\n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    for data, targets in train_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        utils.reset(net) \n",
    "        spk_rec = []\n",
    "        \n",
    "        for step in range(data.size(0)):\n",
    "            spk_out, mem_out = net(data[step])\n",
    "            spk_rec.append(spk_out)\n",
    "\n",
    "\n",
    "        spk_rec = torch.stack(spk_rec)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_loss += loss_val.item()\n",
    "        iter_acc += acc\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "    epoch_loss = iter_loss / counter\n",
    "    epoch_acc = iter_acc / counter\n",
    "    hist['loss'].append(epoch_loss)\n",
    "    hist['acc'].append(epoch_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} \\t Loss: {epoch_loss:.4f} \\t Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Testing...\")\n",
    "net.eval()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        utils.reset(net)\n",
    "        spk_rec = []\n",
    "\n",
    "        for step in range(data.size(0)):\n",
    "            spk_out, mem_out = net(data[step])\n",
    "            spk_rec.append(spk_out)\n",
    "\n",
    "        spk_rec = torch.stack(spk_rec)\n",
    "        \n",
    "        # Calculate correct predictions\n",
    "        # SF.accuracy_rate returns a ratio, so we multiply by batch size to get count\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        correct += acc * data.size(1) \n",
    "        total += data.size(1)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2003cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(hist['loss'], color=color, label=\"Loss\")\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Accuracy', color=color)  \n",
    "ax2.plot(hist['acc'], color=color, label=\"Accuracy\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46052d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch.spikeplot as splt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data, targets = next(iter(test_loader))\n",
    "data = data.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "net.eval()\n",
    "utils.reset(net)\n",
    "spk_rec = []\n",
    "\n",
    "for step in range(data.size(0)):\n",
    "    spk_out, mem_out = net(data[step])\n",
    "    spk_rec.append(spk_out)\n",
    "\n",
    "spk_rec = torch.stack(spk_rec)\n",
    "\n",
    "idx = 0 \n",
    "fig, ax = plt.subplots(facecolor='w', figsize=(12, 8))\n",
    "\n",
    "# Plot the spikes\n",
    "splt.raster(spk_rec[:, idx, :], ax, s=20, c=\"black\")\n",
    "\n",
    "\n",
    "class_labels = train_set.classes\n",
    "ax.set_yticks(range(len(class_labels)))\n",
    "ax.set_yticklabels(class_labels)\n",
    "\n",
    "\n",
    "plt.title(f\"Output Spikes (True Target: {class_labels[targets[idx]]})\")\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.grid(True, linestyle='--', alpha=0.3) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0dc905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaRSNN(nn.Module):\n",
    "    def __init__(self, slope, beta_init, threshold_init, linear_hidden):\n",
    "        super(OptunaRSNN, self).__init__()\n",
    "\n",
    "        # Define Surrogate Gradient using the slope suggested by Optuna\n",
    "        spike_grad = surrogate.atan(alpha=slope)\n",
    "\n",
    "        # --- Deep Convolutional Layers ---\n",
    "        # Note: We initialize beta/threshold with Optuna values, \n",
    "        # but set learn_*=True so the network can fine-tune them.\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(2, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.lif1 = snn.Leaky(beta=beta_init, threshold=threshold_init, \n",
    "                              learn_beta=True, learn_threshold=True,\n",
    "                              spike_grad=spike_grad, init_hidden=True)\n",
    "\n",
    "        # Layer 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.lif2 = snn.Leaky(beta=beta_init, threshold=threshold_init, \n",
    "                              learn_beta=True, learn_threshold=True,\n",
    "                              spike_grad=spike_grad, init_hidden=True)\n",
    "\n",
    "        # Layer 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.lif3 = snn.Leaky(beta=beta_init, threshold=threshold_init, \n",
    "                              learn_beta=True, learn_threshold=True,\n",
    "                              spike_grad=spike_grad, init_hidden=True)\n",
    "                              \n",
    "        # Layer 4\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.lif4 = snn.Leaky(beta=beta_init, threshold=threshold_init, \n",
    "                              learn_beta=True, learn_threshold=True,\n",
    "                              spike_grad=spike_grad, init_hidden=True)\n",
    "\n",
    "        # --- Recurrent Output Block ---\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # We tune the size of this linear layer\n",
    "        self.fc = nn.Linear(128 * 8 * 8, linear_hidden) \n",
    "        \n",
    "        # Recurrent Leaky Layer (RNN)\n",
    "        self.rlif = snn.RLeaky(beta=beta_init, threshold=threshold_init, \n",
    "                               learn_beta=True, learn_threshold=True,\n",
    "                               spike_grad=spike_grad, init_hidden=True,\n",
    "                               linear_features=11, all_to_all=True, output=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.bn1(self.conv1(x)))\n",
    "        x = self.lif1(x)\n",
    "        \n",
    "        x = self.pool2(self.bn2(self.conv2(x)))\n",
    "        x = self.lif2(x)\n",
    "        \n",
    "        x = self.pool3(self.bn3(self.conv3(x)))\n",
    "        x = self.lif3(x)\n",
    "        \n",
    "        x = self.pool4(self.bn4(self.conv4(x)))\n",
    "        x = self.lif4(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        spk_out, mem_out = self.rlif(x)\n",
    "\n",
    "        return spk_out, mem_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416dc320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # --- Suggest Hyperparameters ---\n",
    "    slope = trial.suggest_float(\"slope\", 1.0, 10.0)\n",
    "    beta_init = trial.suggest_float(\"beta_init\", 0.3, 0.95)\n",
    "    threshold_init = trial.suggest_float(\"threshold_init\", 0.5, 2.0)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    linear_hidden = trial.suggest_int(\"linear_hidden\", 64, 256, step=64)\n",
    "    \n",
    "    # --- Setup Model & Training ---\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    \n",
    "    model = OptunaRSNN(slope, beta_init, threshold_init, linear_hidden).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    loss_fn = SF.ce_rate_loss()\n",
    "    \n",
    "    # Use existing loaders (ensure they are defined in your notebook)\n",
    "    # train_loader, test_loader = ... \n",
    "\n",
    "    # --- Fast Training Loop for Optimization ---\n",
    "    epochs = 5 # Keep low for search, train fully with best params later\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            utils.reset(model) # Reset hidden states/memories\n",
    "            \n",
    "            spk_rec = []\n",
    "            for step in range(data.size(0)):\n",
    "                spk_out, mem_out = model(data[step])\n",
    "                spk_rec.append(spk_out)\n",
    "                \n",
    "            spk_rec = torch.stack(spk_rec)\n",
    "            loss = loss_fn(spk_rec, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # --- Validation & Pruning ---\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in test_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                utils.reset(model)\n",
    "                spk_rec = []\n",
    "                for step in range(data.size(0)):\n",
    "                    spk_out, mem_out = model(data[step])\n",
    "                    spk_rec.append(spk_out)\n",
    "                \n",
    "                spk_rec = torch.stack(spk_rec)\n",
    "                acc = SF.accuracy_rate(spk_rec, targets)\n",
    "                correct += acc * data.size(1)\n",
    "                total += data.size(1)\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        \n",
    "        # Report to Optuna\n",
    "        trial.report(val_acc, epoch)\n",
    "        \n",
    "        # Pruning: Stop training if this trial is doing poorly compared to others\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=20) # Start with 20 trials\n",
    "\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "print(\"Best Accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = {\n",
    "#     \"beta_init\": 0.82,      # Example value\n",
    "#     \"slope\": 4.5,           # Example value\n",
    "#     \"lr\": 0.001,            # Example value\n",
    "#     \"linear_hidden\": 128,   # Example value\n",
    "#     \"threshold_init\": 1.0   # Example value (if you optimized it)\n",
    "# }\n",
    "\n",
    "# BATCH_SIZE = 16 \n",
    "# EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958411dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net1 = DeepRSNN(\n",
    "#     slope=best_params[\"slope\"],\n",
    "#     beta_init=best_params[\"beta_init\"],\n",
    "#     threshold_init=best_params[\"threshold_init\"],\n",
    "#     linear_hidden=best_params[\"linear_hidden\"]\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12916c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(net.parameters(), lr=best_params[\"lr\"], betas=(0.9, 0.999))\n",
    "# loss_fn = SF.ce_rate_loss()\n",
    "\n",
    "# print(f\"--- Final Model Initialized ---\")\n",
    "# print(f\"Parameters: {sum(p.numel() for p in net1.parameters())}\")\n",
    "# print(f\"Hyperparams: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc = 0.0\n",
    "# loss_hist = []\n",
    "# test_acc_hist = []\n",
    "\n",
    "# print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     # --- Training ---\n",
    "#     net1.train()\n",
    "#     epoch_loss = 0\n",
    "#     for data, targets in train_loader:\n",
    "#         data, targets = data.to(device), targets.to(device)\n",
    "#         utils.reset(net)\n",
    "#         spk_rec = []\n",
    "\n",
    "#         # Time-loop\n",
    "#         for step in range(data.size(0)):\n",
    "#             spk_out, _ = net(data[step])\n",
    "#             spk_rec.append(spk_out)\n",
    "\n",
    "#         spk_rec = torch.stack(spk_rec)\n",
    "#         loss = loss_fn(spk_rec, targets)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     # --- Validation ---\n",
    "#     net1.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, targets in test_loader:\n",
    "#             data, targets = data.to(device), targets.to(device)\n",
    "#             utils.reset(net)\n",
    "#             spk_rec = []\n",
    "\n",
    "#             for step in range(data.size(0)):\n",
    "#                 spk_out, _ = net(data[step])\n",
    "#                 spk_rec.append(spk_out)\n",
    "\n",
    "#             spk_rec = torch.stack(spk_rec)\n",
    "#             acc = SF.accuracy_rate(spk_rec, targets)\n",
    "#             correct += acc * data.size(1)\n",
    "#             total += data.size(1)\n",
    "\n",
    "#     # --- Stats & Saving ---\n",
    "#     epoch_acc = correct / total\n",
    "#     avg_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "#     loss_hist.append(avg_loss)\n",
    "#     test_acc_hist.append(epoch_acc)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS} \\t Loss: {avg_loss:.4f} \\t Acc: {epoch_acc*100:.2f}%\")\n",
    "\n",
    "#     # Save Best Model\n",
    "#     if epoch_acc > best_acc:\n",
    "#         best_acc = epoch_acc\n",
    "#         torch.save(net.state_dict(), \"best_dvs_gesture_model.pth\")\n",
    "#         print(f\"  --> New Best Model Saved! ({best_acc*100:.2f}%)\")\n",
    "\n",
    "# print(\"Training Complete.\")\n",
    "# print(f\"Highest Accuracy Achieved: {best_acc*100:.2f}%\")\n",
    "# print(\"Best model weights saved to 'best_dvs_gesture_model.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
