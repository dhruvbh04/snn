{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c9f6d79-1564-4fd9-8d2f-e141e98dded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Torch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate, functional as SF, utils\n",
    "from snntorch import spikeplot as splt\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0fe951e-00e3-423f-bb7f-3128e6c41973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Torch_env/lib/python3.10/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([16, 30, 2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "sensor_size = tonic.datasets.DVSGesture.sensor_size\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToFrame(sensor_size=sensor_size, n_time_bins=30),\n",
    "])\n",
    "\n",
    "train_set = tonic.datasets.DVSGesture(save_to='./data', train=True, transform=transform)\n",
    "test_set = tonic.datasets.DVSGesture(save_to='./data', train=False, transform=transform)\n",
    "\n",
    "# Dataloaders\n",
    "cached_dataloader_args = {\n",
    "    \"batch_size\": 16,\n",
    "    \"collate_fn\": tonic.collation.PadTensors(batch_first=True), \n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 2,\n",
    "    \"pin_memory\": True\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train_set, **cached_dataloader_args)\n",
    "test_loader = DataLoader(test_set, **cached_dataloader_args)\n",
    "\n",
    "data, targets = next(iter(train_loader))\n",
    "print(f\"Data shape: {data.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a4469ec-e6ab-4780-844c-aabe72747783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d02f081e-461c-4f4b-89d5-ee673cabd403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>> S-ResNet NM for DVSGesture >>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "class SResnetNM(nn.Module):\n",
    "    def __init__(self, n, nFilters, num_steps, leak_mem=0.95, num_cls=11):\n",
    "        super(SResnetNM, self).__init__()\n",
    "\n",
    "        self.n = n\n",
    "        self.num_cls = num_cls\n",
    "        self.num_steps = num_steps\n",
    "        self.leak_mem = leak_mem\n",
    "\n",
    "        print(\">>>>>>>>>>>>>>>>>>> S-ResNet NM for DVSGesture >>>>>>>>>>>>>>>>>>>>>>\")\n",
    "\n",
    "        affine_flag = True\n",
    "        bias_flag = False\n",
    "        self.nFilters = nFilters\n",
    "\n",
    "        # Define spike gradient surrogate\n",
    "        self.spike_grad = surrogate.atan()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.conv1 = nn.Conv2d(2, self.nFilters, kernel_size=3, stride=2, padding=1, bias=bias_flag)\n",
    "        self.bn1 = nn.BatchNorm2d(self.nFilters, eps=1e-4, momentum=0.1, affine=affine_flag)\n",
    "        self.lif1 = snn.Leaky(beta=leak_mem, spike_grad=self.spike_grad, init_hidden=True)\n",
    "\n",
    "        # Store layers in lists\n",
    "        self.conv_list = nn.ModuleList([self.conv1])\n",
    "        self.bn_list = nn.ModuleList([self.bn1])\n",
    "        self.lif_list = nn.ModuleList([self.lif1])\n",
    "\n",
    "        # Create ResNet blocks\n",
    "        layer_idx = 0\n",
    "        self.block_sizes = []\n",
    "        for block in range(3):\n",
    "            num_layers = 2 * n\n",
    "            self.block_sizes.append(num_layers)\n",
    "            for layer in range(num_layers):\n",
    "                if block != 0 and layer == 0:\n",
    "                    stride = 2\n",
    "                    in_channels = self.nFilters * (2 ** (block - 1))\n",
    "                else:\n",
    "                    stride = 1\n",
    "                    in_channels = self.nFilters * (2 ** block)\n",
    "                \n",
    "                out_channels = self.nFilters * (2 ** block)\n",
    "                \n",
    "                conv = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                                stride=stride, padding=1, bias=bias_flag)\n",
    "                bn = nn.BatchNorm2d(out_channels, eps=1e-4,\n",
    "                                   momentum=0.1, affine=affine_flag)\n",
    "                lif = snn.Leaky(beta=leak_mem, spike_grad=self.spike_grad, init_hidden=True)\n",
    "                \n",
    "                self.conv_list.append(conv)\n",
    "                self.bn_list.append(bn)\n",
    "                self.lif_list.append(lif)\n",
    "                layer_idx += 1\n",
    "\n",
    "        # Skip connection resize layers for downsampling\n",
    "        self.conv_resize_1 = nn.Conv2d(self.nFilters, self.nFilters * 2,\n",
    "                                      kernel_size=1, stride=2, padding=0, bias=bias_flag)\n",
    "        self.resize_bn_1 = nn.BatchNorm2d(self.nFilters * 2, eps=1e-4,\n",
    "                                         momentum=0.1, affine=affine_flag)\n",
    "        self.conv_resize_2 = nn.Conv2d(self.nFilters * 2, self.nFilters * 4,\n",
    "                                      kernel_size=1, stride=2, padding=0, bias=bias_flag)\n",
    "        self.resize_bn_2 = nn.BatchNorm2d(self.nFilters * 4, eps=1e-4,\n",
    "                                         momentum=0.1, affine=affine_flag)\n",
    "        \n",
    "        # Spiking neurons for resize layers\n",
    "        self.lif_resize_1 = snn.Leaky(beta=leak_mem, spike_grad=self.spike_grad, init_hidden=True)\n",
    "        self.lif_resize_2 = snn.Leaky(beta=leak_mem, spike_grad=self.spike_grad, init_hidden=True)\n",
    "\n",
    "        # Adaptive pooling to handle variable spatial dimensions\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(self.nFilters * 4, self.num_cls, bias=bias_flag)\n",
    "        self.lif_out = snn.Leaky(beta=leak_mem, spike_grad=self.spike_grad, init_hidden=True, output=True)\n",
    "\n",
    "        self.conv1x1_list = nn.ModuleList([self.conv_resize_1, self.conv_resize_2])\n",
    "        self.bn_conv1x1_list = nn.ModuleList([self.resize_bn_1, self.resize_bn_2])\n",
    "        self.lif_conv1x1_list = nn.ModuleList([self.lif_resize_1, self.lif_resize_2])\n",
    "\n",
    "        # Turn off bias of BatchNorm\n",
    "        for bn_temp in self.bn_list:\n",
    "            if hasattr(bn_temp, 'bias') and bn_temp.bias is not None:\n",
    "                bn_temp.bias = None\n",
    "        for bn_temp in self.bn_conv1x1_list:\n",
    "            if hasattr(bn_temp, 'bias') and bn_temp.bias is not None:\n",
    "                bn_temp.bias = None\n",
    "\n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=2)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=2)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Input shape: [N, T, 2, H, W] (batch_first=True)\n",
    "        batch_size = inp.size(0)\n",
    "        num_timesteps = inp.size(1)\n",
    "        \n",
    "        # Initialize lists to store outputs\n",
    "        spk_out_list = []\n",
    "        \n",
    "        # Process each timestep\n",
    "        for t in range(num_timesteps):\n",
    "            # Get current timestep input\n",
    "            x = inp[:, t]  # [N, 2, H, W]\n",
    "            \n",
    "            # Resize from 128x128 to 64x64 if needed\n",
    "            if x.shape[-1] == 128:\n",
    "                x = F.interpolate(x, size=64, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Initial conv block\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            spike, mem = self.lif1(x)\n",
    "            \n",
    "            skip = spike.clone()\n",
    "            x = spike\n",
    "            \n",
    "            # Track skip connection index\n",
    "            skip_idx = 0\n",
    "            conv_idx = 1  # Start after first conv (index 0)\n",
    "            \n",
    "            # Process ResNet blocks\n",
    "            block_start_idx = 0\n",
    "            for block in range(3):\n",
    "                block_layers = self.block_sizes[block]\n",
    "                \n",
    "                for layer_in_block in range(block_layers):\n",
    "                    # Get current layer\n",
    "                    conv = self.conv_list[conv_idx]\n",
    "                    bn = self.bn_list[conv_idx]\n",
    "                    lif = self.lif_list[conv_idx]\n",
    "                    \n",
    "                    # Forward through layer\n",
    "                    x = conv(x)\n",
    "                    x = bn(x)\n",
    "                    spike, mem = lif(x)\n",
    "                    \n",
    "                    # Add skip connection at the END of each block (except first layer of block)\n",
    "                    if layer_in_block == block_layers - 1:  # Last layer in block\n",
    "                        # For blocks after the first one, we need to process skip connection\n",
    "                        if block > 0:\n",
    "                            # Get the skip connection processing layer\n",
    "                            skip_processed = self.conv1x1_list[skip_idx](skip)\n",
    "                            skip_processed = self.bn_conv1x1_list[skip_idx](skip_processed)\n",
    "                            skip_spike, _ = self.lif_conv1x1_list[skip_idx](skip_processed)\n",
    "                            spike = spike + skip_spike\n",
    "                            skip_idx += 1\n",
    "                        else:\n",
    "                            # For first block, just add\n",
    "                            spike = spike + skip\n",
    "                        \n",
    "                        # Update skip for next block\n",
    "                        skip = spike.clone()\n",
    "                    \n",
    "                    x = spike\n",
    "                    conv_idx += 1\n",
    "            \n",
    "            # Final pooling and classification\n",
    "            x = self.pool(x)\n",
    "            x = x.view(batch_size, -1)\n",
    "            x = self.fc(x)\n",
    "            spk_out, mem_out = self.lif_out(x)\n",
    "            \n",
    "            spk_out_list.append(spk_out)\n",
    "        \n",
    "        # Stack outputs across time\n",
    "        spk_out = torch.stack(spk_out_list, dim=0)  # [T, N, num_cls]\n",
    "        \n",
    "        return spk_out\n",
    "\n",
    "net = SResnetNM(n=3, nFilters=16, num_steps=30, num_cls=11).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f01fa0f-6f1d-46ff-9bdf-7fc8ef4234ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
    "loss_fn = SF.ce_rate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b68e4d-f2c6-4461-b44c-745fc139f1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|                                       | 0/68 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m utils\u001b[38;5;241m.\u001b[39mreset(net)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Forward pass - process all timesteps\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m spk_rec \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [T, N, num_cls]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate loss and accuracy\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m loss_fn(spk_rec, targets)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 114\u001b[0m, in \u001b[0;36mSResnetNM.forward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    112\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m    113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m--> 114\u001b[0m spike, mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif1(x)\n\u001b[1;32m    116\u001b[0m skip \u001b[38;5;241m=\u001b[39m spike\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    117\u001b[0m x \u001b[38;5;241m=\u001b[39m spike\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "hist = {\"loss\": [], \"acc\": []}\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    iter_loss = 0\n",
    "    iter_acc = 0\n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    # Add tqdm for progress bar\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for data, targets in pbar:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Reset states for this batch\n",
    "        utils.reset(net)\n",
    "        \n",
    "        # Forward pass - process all timesteps\n",
    "        spk_rec = net(data)  # [T, N, num_cls]\n",
    "        \n",
    "        # Calculate loss and accuracy\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_loss += loss_val.item()\n",
    "        iter_acc += acc\n",
    "        counter += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss_val.item():.4f}',\n",
    "            'acc': f'{acc:.4f}'\n",
    "        })\n",
    "\n",
    "    epoch_loss = iter_loss / counter\n",
    "    epoch_acc = iter_acc / counter\n",
    "    hist['loss'].append(epoch_loss)\n",
    "    hist['acc'].append(epoch_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} \\t Loss: {epoch_loss:.4f} \\t Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976ab87-8d7a-4f34-a1a4-9986e3bfb9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Testing...\")\n",
    "net.eval()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in tqdm(test_loader, desc=\"Testing\"):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Reset states\n",
    "        utils.reset(net)\n",
    "        \n",
    "        # Forward pass\n",
    "        spk_rec = net(data)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        correct += acc * data.size(0)  # data.size(0) is batch size\n",
    "        total += data.size(0)\n",
    "\n",
    "test_acc = correct / total if total > 0 else 0\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a85471-cb1f-459f-b51d-2d15bc91673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist['acc'])\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893a64e-5d63-471c-b7eb-5153a952b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(hist['loss'], color=color, label=\"Loss\")\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Accuracy', color=color)  \n",
    "ax2.plot(hist['acc'], color=color, label=\"Accuracy\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f9f3bb-fe99-4d28-b10d-191b5d38ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "def play_anim(data, labels, index=0):\n",
    "    \"\"\"\n",
    "    data: Tensor of shape (Time, Batch, Channels, Height, Width)\n",
    "    labels: Target integers\n",
    "    index: Which sample in the batch to visualize\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the specific sample from the batch: (Time, Channels, Height, Width)\n",
    "    # We detach from graph and move to cpu for plotting\n",
    "    sample = data[:, index, :, :, :].cpu()\n",
    "    label_id = labels[index].item()\n",
    "    class_name = train_set.classes[label_id]\n",
    "\n",
    "    # Combine channels for visualization\n",
    "    # Channel 1 (Positive) -> Red, Channel 0 (Negative) -> Blue\n",
    "    # We create a single image where positive is +1 and negative is -1\n",
    "    frames = sample[:, 1, :, :] - sample[:, 0, :, :]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(frames[0], cmap='seismic', vmin=-1.5, vmax=1.5)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Label: {class_name}\")\n",
    "\n",
    "    def update(frame_idx):\n",
    "        # Update the image data for the next frame\n",
    "        im.set_data(frames[frame_idx])\n",
    "        return [im]\n",
    "\n",
    "    # Create animation\n",
    "    # frames=sample.shape[0] ensures we loop through all 15 time steps\n",
    "    ani = animation.FuncAnimation(fig, update, frames=sample.shape[0], interval=200, blit=True)\n",
    "    \n",
    "    plt.close() # Prevent static plot from showing up separately\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b763eba-5b5a-4c18-a375-5c29e070aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get a batch of data\n",
    "data_batch, targets_batch = next(iter(test_loader))\n",
    "\n",
    "# 2. Generate the animation for the first sample in the batch (index 0)\n",
    "anim = play_anim(data_batch, targets_batch, index=0)\n",
    "\n",
    "# 3. Render it in the notebook\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e2a8f-e977-42d5-933a-15bc96cae613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch.spikeplot as splt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data, targets = next(iter(test_loader))\n",
    "data = data.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "net.eval()\n",
    "utils.reset(net)\n",
    "spk_rec = []\n",
    "\n",
    "for step in range(data.size(0)):\n",
    "    spk_out, mem_out = net(data[step])\n",
    "    spk_rec.append(spk_out)\n",
    "\n",
    "spk_rec = torch.stack(spk_rec)\n",
    "\n",
    "idx = 0 \n",
    "fig, ax = plt.subplots(facecolor='w', figsize=(12, 8))\n",
    "\n",
    "# Plot the spikes\n",
    "splt.raster(spk_rec[:, idx, :], ax, s=20, c=\"black\")\n",
    "\n",
    "\n",
    "class_labels = train_set.classes\n",
    "ax.set_yticks(range(len(class_labels)))\n",
    "ax.set_yticklabels(class_labels)\n",
    "\n",
    "\n",
    "plt.title(f\"Output Spikes (True Target: {class_labels[targets[idx]]})\")\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.grid(True, linestyle='--', alpha=0.3) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba22be-0b1d-4f68-aa3f-73d59f17c74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
