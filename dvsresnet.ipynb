{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ef8f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "from tonic import DiskCachedDataset\n",
    "from tonic.dataset import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from collections.abc import Callable\n",
    "\n",
    "import optuna\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02d515d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([30, 16, 2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "sensor_size = tonic.datasets.DVSGesture.sensor_size\n",
    "\n",
    "# 15 time steps\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToFrame(sensor_size=sensor_size, n_time_bins=30),\n",
    "])\n",
    "\n",
    "train_set = tonic.datasets.DVSGesture(save_to='./data', train=True, transform=transform)\n",
    "test_set = tonic.datasets.DVSGesture(save_to='./data', train=False, transform=transform)\n",
    "\n",
    "# Dataloaders\n",
    "cached_dataloader_args = {\n",
    "    \"batch_size\": 16,\n",
    "    \"collate_fn\": tonic.collation.PadTensors(batch_first=False), \n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 1,\n",
    "    \"pin_memory\": True\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train_set, **cached_dataloader_args)\n",
    "test_loader = DataLoader(test_set, **cached_dataloader_args)\n",
    "\n",
    "data, targets = next(iter(train_loader))\n",
    "print(f\"Data shape: {data.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85d61026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: xpu\n"
     ]
    }
   ],
   "source": [
    "if torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58d58902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global neuron parameters\n",
    "beta = 0.5  \n",
    "spike_grad = surrogate.atan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7545793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2. ResNet Building Blocks (Adapted for Single-Step)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def conv3x3(in_channels, out_channels, beta, spike_grad):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "    )\n",
    "\n",
    "def conv1x1(in_channels, out_channels, beta, spike_grad):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "    )\n",
    "\n",
    "class SEWBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, connect_f, beta, spike_grad):\n",
    "        super(SEWBlock, self).__init__()\n",
    "        self.connect_f = connect_f\n",
    "        self.conv = nn.Sequential(\n",
    "            conv3x3(in_channels, mid_channels, beta, spike_grad),\n",
    "            conv3x3(mid_channels, in_channels, beta, spike_grad),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        if self.connect_f == 'ADD':\n",
    "            out = out + x\n",
    "        elif self.connect_f == 'AND':\n",
    "            out = out * x\n",
    "        elif self.connect_f == 'IAND':\n",
    "            out = x * (1. - out)\n",
    "        else:\n",
    "            raise NotImplementedError(self.connect_f)\n",
    "        return out\n",
    "\n",
    "class PlainBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, beta, spike_grad):\n",
    "        super(PlainBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            conv3x3(in_channels, mid_channels, beta, spike_grad),\n",
    "            conv3x3(mid_channels, in_channels, beta, spike_grad),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, beta, spike_grad):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            conv3x3(in_channels, mid_channels, beta, spike_grad),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(mid_channels, in_channels, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "                nn.BatchNorm2d(in_channels),\n",
    "            ),\n",
    "        )\n",
    "        self.sn = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sn(x + self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f0d59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3. Main Network Class\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class ResNetN(nn.Module):\n",
    "    def __init__(self, layer_list, num_classes, connect_f=None, beta=0.5, spike_grad=None):\n",
    "        super(ResNetN, self).__init__()\n",
    "        \n",
    "        in_channels = 2 # DVS has 2 channels (on/off)\n",
    "        conv = []\n",
    "\n",
    "        # --- Build Layers ---\n",
    "        for cfg_dict in layer_list:\n",
    "            channels = cfg_dict['channels']\n",
    "            mid_channels = cfg_dict.get('mid_channels', channels)\n",
    "\n",
    "            # 1. Down/Up Sampling if channels change\n",
    "            if in_channels != channels:\n",
    "                if cfg_dict['up_kernel_size'] == 3:\n",
    "                    conv.append(conv3x3(in_channels, channels, beta, spike_grad))\n",
    "                elif cfg_dict['up_kernel_size'] == 1:\n",
    "                    conv.append(conv1x1(in_channels, channels, beta, spike_grad))\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "            \n",
    "            in_channels = channels\n",
    "\n",
    "            # 2. Residual Blocks\n",
    "            if 'num_blocks' in cfg_dict:\n",
    "                for _ in range(cfg_dict['num_blocks']):\n",
    "                    if cfg_dict['block_type'] == 'sew':\n",
    "                        conv.append(SEWBlock(in_channels, mid_channels, connect_f, beta, spike_grad))\n",
    "                    elif cfg_dict['block_type'] == 'plain':\n",
    "                        conv.append(PlainBlock(in_channels, mid_channels, beta, spike_grad))\n",
    "                    elif cfg_dict['block_type'] == 'basic':\n",
    "                        conv.append(BasicBlock(in_channels, mid_channels, beta, spike_grad))\n",
    "\n",
    "            # 3. Pooling\n",
    "            if 'k_pool' in cfg_dict:\n",
    "                conv.append(nn.MaxPool2d(cfg_dict['k_pool'], cfg_dict['k_pool']))\n",
    "\n",
    "        # Flatten features before linear layer\n",
    "        conv.append(nn.Flatten(1)) \n",
    "\n",
    "        self.conv = nn.Sequential(*conv)\n",
    "\n",
    "        # --- Calculate Feature Size Dynamically ---\n",
    "        # We run a dummy pass to see how many features come out of the conv stack\n",
    "        with torch.no_grad():\n",
    "            dummy_x = torch.zeros([1, 2, 128, 128])\n",
    "            # Since our layers use init_hidden=True, we can pass a single frame safely\n",
    "            # Note: The output will be [Batch, Features]\n",
    "            out_features = self.conv(dummy_x).shape[1]\n",
    "\n",
    "        # --- Output Head ---\n",
    "        self.fc = nn.Linear(out_features, num_classes, bias=True)\n",
    "        # Final neuron returns (spk, mem)\n",
    "        self.final_lif = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Processes a SINGLE time-step.\n",
    "        Input x: [Batch, Channel, Height, Width]\n",
    "        \"\"\"\n",
    "        # Pass through the Conv Blocks (state is handled internally by init_hidden=True)\n",
    "        features = self.conv(x)\n",
    "        \n",
    "        # Pass through Linear\n",
    "        cur = self.fc(features)\n",
    "        \n",
    "        # Pass through Final LIF\n",
    "        spk, mem = self.final_lif(cur)\n",
    "        \n",
    "        return spk, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb4c5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4. Model Wrapper (The SEW ResNet Configuration)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def SEWResNet(connect_f='ADD', num_classes=11, beta=0.5, spike_grad=None):\n",
    "    # This configuration defines the depth and width of the ResNet\n",
    "    layer_list = [\n",
    "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
    "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
    "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
    "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
    "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
    "    ]\n",
    "    \n",
    "    return ResNetN(layer_list, num_classes, connect_f, beta, spike_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9359a5ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Native API failed. Native API returns: 2147483646 (UR_RESULT_ERROR_UNKNOWN)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 5. Initialization & Training Loop\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize Model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mSEWResNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnect_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspike_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspike_grad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-3\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m))\n\u001b[0;32m      9\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m SF\u001b[38;5;241m.\u001b[39mce_rate_loss()\n",
      "File \u001b[1;32mc:\\Users\\dhruv\\miniconda3\\envs\\jupyter_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dhruv\\miniconda3\\envs\\jupyter_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhruv\\miniconda3\\envs\\jupyter_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhruv\\miniconda3\\envs\\jupyter_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhruv\\miniconda3\\envs\\jupyter_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhruv\\miniconda3\\envs\\jupyter_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1336\u001b[0m             device,\n\u001b[0;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1338\u001b[0m             non_blocking,\n\u001b[0;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1340\u001b[0m         )\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Native API failed. Native API returns: 2147483646 (UR_RESULT_ERROR_UNKNOWN)"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 5. Initialization & Training Loop\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Initialize Model\n",
    "net = SEWResNet(connect_f='ADD', num_classes=11, beta=beta, spike_grad=spike_grad).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
    "loss_fn = SF.ce_rate_loss()\n",
    "\n",
    "num_epochs = 10\n",
    "hist = {\"loss\": [], \"acc\": []}\n",
    "\n",
    "print(\"Starting Training with SEW ResNet...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    iter_loss = 0\n",
    "    iter_acc = 0\n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    # 2. Wrap train_loader with tqdm\n",
    "    # 'desc' sets the text at the start of the bar\n",
    "    # 'unit' defines the label for iterations\n",
    "    with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "        \n",
    "        for data, targets in pbar:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            utils.reset(net) \n",
    "            spk_rec = []\n",
    "            \n",
    "            # Time Loop\n",
    "            for step in range(data.size(0)):\n",
    "                spk_out, mem_out = net(data[step])\n",
    "                spk_rec.append(spk_out)\n",
    "\n",
    "            spk_rec = torch.stack(spk_rec)\n",
    "            \n",
    "            # Loss & Backprop\n",
    "            loss_val = loss_fn(spk_rec, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accuracy\n",
    "            acc = SF.accuracy_rate(spk_rec, targets)\n",
    "\n",
    "            iter_loss += loss_val.item()\n",
    "            iter_acc += acc\n",
    "            counter += 1\n",
    "            \n",
    "            # 3. Update the progress bar with current stats\n",
    "            # This updates the text on the right side of the bar dynamically\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{iter_loss/counter:.4f}\", \n",
    "                \"Acc\": f\"{iter_acc/counter:.4f}\"\n",
    "            })\n",
    "\n",
    "    # End of epoch stats\n",
    "    epoch_loss = iter_loss / counter\n",
    "    epoch_acc = iter_acc / counter\n",
    "    hist['loss'].append(epoch_loss)\n",
    "    hist['acc'].append(epoch_acc)\n",
    "    \n",
    "    # Optional: Print total time for epoch (tqdm handles time too, but if you want explicit log)\n",
    "    print(f\"Epoch {epoch+1} finished in {time.time()-t0:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
