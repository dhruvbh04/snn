{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8597152-d97b-4e72-ba48-1a356da60485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: xpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tonic for data loading (Using your existing method)\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "\n",
    "# Check Device\n",
    "if torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f53ab4-1ea9-48bc-ae81-785f735fabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.T = 20          # Time windows (Time steps)\n",
    "        self.dt = 1          # Time resolution\n",
    "        self.b = 8          # Batch size\n",
    "        self.lr = 1e-4       # Learning rate\n",
    "        self.epochs = 50     # Number of epochs\n",
    "        \n",
    "        # Neuron Parameters from the repo\n",
    "        self.alpha = 0.3     # Decay factor\n",
    "        self.beta = 0.       # (Not used in this repo's specific LIF usually)\n",
    "        self.Vreset = 0.     # Reset voltage\n",
    "        self.Vthres = 0.3    # Threshold\n",
    "        \n",
    "        # Architecture\n",
    "        self.target_size = 11 # DVS Gesture classes\n",
    "        self.reduction = 16   # Attention reduction ratio\n",
    "        self.mode_select = 'spike' # 'spike' or 'mem'\n",
    "        \n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d575b1-63b2-49bd-8b06-339a698fd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Temporal Attention Layer (TA) ---\n",
    "class Tlayer(nn.Module):\n",
    "    def __init__(self, timeWindows, reduction=16, dimension=4):\n",
    "        super(Tlayer, self).__init__()\n",
    "        # Adapts pooling based on input dimensions (4D for FC, 5D for Conv)\n",
    "        if dimension == 3: self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        elif dimension == 4: self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        else: self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "\n",
    "        self.temporal_excitation = nn.Sequential(\n",
    "            nn.Linear(timeWindows, int(timeWindows // reduction)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(int(timeWindows // reduction), timeWindows),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Input shape: [Batch, Time, Channels, Height, Width] (for Conv)\n",
    "        b, t = input.size(0), input.size(1)\n",
    "        \n",
    "        # Pool spatial dims to get temporal statistics\n",
    "        temp = self.avg_pool(input) \n",
    "        y = temp.view(b, t)\n",
    "        \n",
    "        # Calculate attention weights\n",
    "        y = self.temporal_excitation(y).view(temp.size())\n",
    "        \n",
    "        # Apply weights\n",
    "        return input * y\n",
    "\n",
    "# --- Custom Integrate and Fire Cell ---\n",
    "# Used for both Conv and FC layers in the repo\n",
    "class IFCell(nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, spikeActFun, scale=0.3, pa_dict=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.spikeActFun = spikeActFun\n",
    "        self.pa_dict = pa_dict\n",
    "        self.alpha = pa_dict['alpha']\n",
    "        self.Vreset = pa_dict['Vreset']\n",
    "        self.Vthres = pa_dict['Vthres']\n",
    "        self.h = None # Membrane potential\n",
    "\n",
    "    def forward(self, input, init_v=None):\n",
    "        self.batchSize = input.size(0)\n",
    "        \n",
    "        # Initialize membrane potential if first step\n",
    "        if self.h is None:\n",
    "            if input.dim() == 4: # Conv Layer [B, C, H, W]\n",
    "                 self.h = torch.zeros(self.batchSize, self.hiddenSize, input.size(2), input.size(3)).to(input.device)\n",
    "            else: # FC Layer [B, Features]\n",
    "                 self.h = torch.zeros(self.batchSize, self.hiddenSize).to(input.device)\n",
    "\n",
    "        # LIF Dynamics\n",
    "        u = self.h + input\n",
    "        x_ = u - self.Vthres\n",
    "        x = self.spikeActFun(x_) # Generate Spike\n",
    "        \n",
    "        # Hard Reset (Repo approach) + Leak\n",
    "        # self.h = x * self.Vreset + (1 - x) * u # If using reset to specific value\n",
    "        self.h = x * self.Vthres + (1 - x) * u # Soft reset/Threshold subtraction often used\n",
    "        self.h = self.h * self.alpha # Decay\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.h = None\n",
    "\n",
    "# --- Wrapper for Convolution + Attention + LIF ---\n",
    "class ConvAttLIF(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size, spikeActFun, padding=1, pa_dict=None, reduction=16, T=60, stride=1, pooling_kernel=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_c, out_c, kernel_size, padding=padding, stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "        self.pooling_kernel = pooling_kernel\n",
    "        if pooling_kernel > 1:\n",
    "            self.pool = nn.AvgPool2d(pooling_kernel)\n",
    "        \n",
    "        self.attention = Tlayer(timeWindows=T, dimension=5, reduction=reduction)\n",
    "        self.lif = IFCell(0, out_c, spikeActFun, pa_dict=pa_dict)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Reset neuron state at start of batch\n",
    "        self.lif.reset()\n",
    "        \n",
    "        # Data: [Batch, Time, Channel, Height, Width]\n",
    "        b, t, c, h, w = data.size()\n",
    "        \n",
    "        # 1. Spatial Convolution over all time steps (reshaped)\n",
    "        out = data.reshape(b * t, c, h, w)\n",
    "        out = self.conv(out)\n",
    "        out = self.bn(out)\n",
    "        \n",
    "        if self.pooling_kernel > 1:\n",
    "            out = self.pool(out)\n",
    "        \n",
    "        # Reshape back to separate Time dimension\n",
    "        _, c_out, h_out, w_out = out.size()\n",
    "        out = out.reshape(b, t, c_out, h_out, w_out)\n",
    "        \n",
    "        # 2. Temporal Attention\n",
    "        out = self.attention(out)\n",
    "        \n",
    "        # 3. Integrate and Fire (Loop over time)\n",
    "        output_spikes = []\n",
    "        for step in range(t):\n",
    "            spike = self.lif(out[:, step])\n",
    "            output_spikes.append(spike)\n",
    "            \n",
    "        return torch.stack(output_spikes, dim=1) # [B, T, C, H, W]\n",
    "\n",
    "# --- Wrapper for FC + Attention + LIF ---\n",
    "class FCAttLIF(nn.Module):\n",
    "    def __init__(self, in_features, out_features, spikeActFun, pa_dict=None, reduction=16, T=60):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.attention = Tlayer(timeWindows=T, dimension=3, reduction=reduction)\n",
    "        self.lif = IFCell(0, out_features, spikeActFun, pa_dict=pa_dict)\n",
    "\n",
    "    def forward(self, data):\n",
    "        self.lif.reset()\n",
    "        b, t, _ = data.size()\n",
    "        \n",
    "        # Linear transform\n",
    "        out = self.linear(data.reshape(b*t, -1))\n",
    "        out = self.bn(out)\n",
    "        out = out.reshape(b, t, -1)\n",
    "        \n",
    "        # Attention\n",
    "        out = self.attention(out)\n",
    "        \n",
    "        # LIF\n",
    "        output_spikes = []\n",
    "        for step in range(t):\n",
    "            spike = self.lif(out[:, step])\n",
    "            output_spikes.append(spike)\n",
    "            \n",
    "        return torch.stack(output_spikes, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9212927d-216f-4961-aeac-3c8fa8c13f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActFun(torch.autograd.Function):\n",
    "    \"\"\" Approximate Firing Function (Surrogate Gradient) \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.ge(0.).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        lens = 0.5 \n",
    "        temp = abs(input) < lens\n",
    "        return grad_output * temp.float() / (2 * lens)\n",
    "\n",
    "class TA_SNN_Net(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TA_SNN_Net, self).__init__()\n",
    "        self.cfg = config\n",
    "        pa_dict = {'alpha': config.alpha, 'beta': config.beta, 'Vreset': config.Vreset, 'Vthres': config.Vthres}\n",
    "        \n",
    "        # CNN Configuration: [InCh, OutCh, Kernel, Padding, Pooling]\n",
    "        # Standard DVSGesture structure used in the repo\n",
    "        self.conv1 = ConvAttLIF(2, 64, kernel_size=3, padding=1, stride=1, pooling_kernel=2, \n",
    "                                spikeActFun=ActFun.apply, pa_dict=pa_dict, T=config.T, reduction=config.reduction)\n",
    "        \n",
    "        self.conv2 = ConvAttLIF(64, 128, kernel_size=3, padding=1, stride=1, pooling_kernel=2, \n",
    "                                spikeActFun=ActFun.apply, pa_dict=pa_dict, T=config.T, reduction=config.reduction)\n",
    "        \n",
    "        self.conv3 = ConvAttLIF(128, 128, kernel_size=3, padding=1, stride=1, pooling_kernel=2, \n",
    "                                spikeActFun=ActFun.apply, pa_dict=pa_dict, T=config.T, reduction=config.reduction)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        # 128x128 -> (Pool2) 64 -> (Pool2) 32 -> (Pool2) 16\n",
    "        flat_size = 128 * 16 * 16 \n",
    "        \n",
    "        self.fc1 = FCAttLIF(flat_size, 256, spikeActFun=ActFun.apply, pa_dict=pa_dict, T=config.T, reduction=config.reduction)\n",
    "        self.fc2 = FCAttLIF(256, config.target_size, spikeActFun=ActFun.apply, pa_dict=pa_dict, T=config.T, reduction=config.reduction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch, Time, Channel, H, W]\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        b, t, c, h, w = x.size()\n",
    "        x = x.reshape(b, t, -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Average spikes over time for classification rate\n",
    "        return torch.sum(x, dim=1) / t \n",
    "\n",
    "# Initialize Model\n",
    "model = TA_SNN_Net(cfg).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "criterion = nn.MSELoss() # Repo uses MSE on Spike Rates often, or CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69de50ff-6ba9-4c3e-a291-790e96685d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([8, 20, 2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "sensor_size = tonic.datasets.DVSGesture.sensor_size\n",
    "\n",
    "# Transform: Create Frames. \n",
    "# IMPORTANT: n_time_bins must match cfg.T (60)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToFrame(sensor_size=sensor_size, n_time_bins=cfg.T),\n",
    "])\n",
    "\n",
    "train_set = tonic.datasets.DVSGesture(save_to='./data', train=True, transform=transform)\n",
    "test_set = tonic.datasets.DVSGesture(save_to='./data', train=False, transform=transform)\n",
    "\n",
    "cached_dataloader_args = {\n",
    "    \"batch_size\": cfg.b,\n",
    "    \"collate_fn\": tonic.collation.PadTensors(batch_first=True), # Note: batch_first=True needed for [B, T, C, H, W]\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 2,\n",
    "    \"drop_last\": True\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train_set, **cached_dataloader_args)\n",
    "test_loader = DataLoader(test_set, **cached_dataloader_args)\n",
    "\n",
    "# Verification\n",
    "d, t = next(iter(train_loader))\n",
    "print(f\"Input Shape: {d.shape}\") # Should be [16, 60, 2, 128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e9e00-ac37-4740-b43d-083d2688c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|                                                                              | 0/134 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{cfg.epochs}\")\n",
    "    for i, (images, labels) in enumerate(pbar):\n",
    "        images = images.float().to(device)\n",
    "        \n",
    "        # One-hot encode labels for MSE Loss (Common in this repo style)\n",
    "        # Or use CrossEntropy on the output rates\n",
    "        target = torch.zeros(images.size(0), cfg.target_size).to(device)\n",
    "        target.scatter_(1, labels.unsqueeze(1).long().to(device), 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images) # Returns spike rates [B, Classes]\n",
    "        \n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'Loss': train_loss/(i+1), 'Acc': 100*correct/total})\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "# --- Run ---\n",
    "best_acc = 0\n",
    "for epoch in range(1, cfg.epochs + 1):\n",
    "    train(epoch)\n",
    "    acc = test(epoch)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        # torch.save(model.state_dict(), 'best_ta_snn.pth')\n",
    "        print(f\"New Best Accuracy: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fab4e-0afc-4709-93d0-954936e05d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
